# AI Crawler Assistant

ğŸ¤– æ™ºèƒ½åŒ–çš„æ‹›è˜ç½‘ç«™çˆ¬è™«å·¥å…·ï¼Œä½¿ç”¨ AI æŠ€æœ¯è‡ªåŠ¨è¯†åˆ«é¡µé¢ç»“æ„å¹¶æå–èŒä½ä¿¡æ¯ã€‚

## ğŸŒŸ ç‰¹æ€§

- **ğŸ§  AI é©±åŠ¨**: ä½¿ç”¨ GPT-4 è‡ªåŠ¨åˆ†æé¡µé¢ç»“æ„ï¼Œç”Ÿæˆå‡†ç¡®çš„ CSS é€‰æ‹©å™¨
- **ğŸŒ è‡ªé€‚åº”**: æ”¯æŒä¸åŒæ‹›è˜ç½‘ç«™çš„è‡ªåŠ¨é€‚é…ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
- **ğŸ” æ™ºèƒ½æå–**: è‡ªåŠ¨è¯†åˆ«èŒä½æ ‡é¢˜ã€å…¬å¸åç§°ã€åœ°ç‚¹ã€æè¿°ç­‰å…³é”®ä¿¡æ¯
- **ğŸ”„ åˆ†é¡µå¤„ç†**: æ™ºèƒ½å¤„ç†å„ç§åˆ†é¡µæ¨¡å¼ï¼Œæ”¯æŒ"ä¸‹ä¸€é¡µ"ã€"åŠ è½½æ›´å¤š"ç­‰
- **ğŸ›¡ï¸ åçˆ¬è™«**: ä½¿ç”¨ browser-use æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼Œç»•è¿‡åçˆ¬è™«æ£€æµ‹
- **ğŸ“Š æ•°æ®è´¨é‡**: å†…ç½®æ•°æ®æ¸…æ´—å’ŒéªŒè¯æœºåˆ¶ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§
- **âš¡ é«˜æ€§èƒ½**: æ”¯æŒå¹¶å‘çˆ¬å–ï¼Œå¼‚æ­¥å¤„ç†ï¼Œæé«˜æ•ˆç‡
- **ğŸ“ˆ ç›‘æ§**: å®Œæ•´çš„æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ç”¨æˆ·æ¥å£å±‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           APIç½‘å…³å±‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æ ¸å¿ƒä¸šåŠ¡å±‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ AIåˆ†ææ¨¡å—  â”‚  çˆ¬è™«å¼•æ“       â”‚   â”‚
â”‚  â”‚             â”‚                 â”‚   â”‚
â”‚  â”‚ é¡µé¢åˆ†æ    â”‚  Browser-use    â”‚   â”‚
â”‚  â”‚ é€‰æ‹©å™¨ç”Ÿæˆ  â”‚  æ•°æ®æå–       â”‚   â”‚
â”‚  â”‚ æ™ºèƒ½éªŒè¯    â”‚  åˆ†é¡µå¤„ç†       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æ•°æ®å­˜å‚¨å±‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PostgreSQL  â”‚     Redis       â”‚   â”‚
â”‚  â”‚             â”‚                 â”‚   â”‚
â”‚  â”‚ ç»“æ„åŒ–æ•°æ®  â”‚   ç¼“å­˜&ä¼šè¯     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.11+
- Docker & Docker Compose
- OpenAI API Key

### 1. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/your-username/ai-crawler-assistant.git
cd ai-crawler-assistant
```

### 2. ç¯å¢ƒé…ç½®

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ç¯å¢ƒå˜é‡
vim .env
```

åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®ï¼š

```env
# OpenAIé…ç½®
OPENAI_API_KEY=your_openai_api_key_here

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://postgres:password@localhost:5432/crawler_db
REDIS_URL=redis://localhost:6379

# åº”ç”¨é…ç½®
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=true

# Browser-useé…ç½®
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=30000

# ä»£ç†é…ç½®ï¼ˆå¯é€‰ï¼‰
PROXY_ENABLED=false
PROXY_LIST=proxy1:port1,proxy2:port2
```

### 3. å¯åŠ¨æœåŠ¡

```bash
# ä½¿ç”¨Docker Composeå¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æˆ–è€…æœ¬åœ°å¼€å‘æ¨¡å¼
python -m pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 4. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥APIå¥åº·çŠ¶æ€
curl http://localhost:8000/health

# æŸ¥çœ‹APIæ–‡æ¡£
open http://localhost:8000/docs
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ä½¿ç”¨æµç¨‹

#### 1. åˆ†æé¡µé¢ç»“æ„

```python
import requests

# æäº¤è¦åˆ†æçš„æ‹›è˜é¡µé¢URL
response = requests.post("http://localhost:8000/api/v1/analyze-url", json={
    "url": "https://example-job-site.com/jobs",
    "options": {
        "wait_time": 3,  # ç­‰å¾…é¡µé¢åŠ è½½æ—¶é—´(ç§’)
        "include_screenshots": true  # æ˜¯å¦åŒ…å«æˆªå›¾
    }
})

selectors = response.json()
print(selectors)
```

**è¿”å›ç¤ºä¾‹**:

```json
{
  "selectors": {
    "jobList": ".jobs-container",
    "jobItem": ".job-card",
    "jobTitle": "h3.job-title a",
    "jobLink": "h3.job-title a",
    "companyName": ".company-name",
    "publishedAt": ".publish-date",
    "location": ".job-location",
    "jobDescription": ".job-summary",
    "nextPage": ".pagination .next-page"
  },
  "confidence_score": 0.95,
  "validation_details": {
    "single_page_accuracy": 0.98,
    "multi_page_stability": 0.92,
    "data_completeness": 0.94
  }
}
```

#### 2. å¯åŠ¨çˆ¬è™«ä»»åŠ¡

```python
# ä½¿ç”¨ç”Ÿæˆçš„é€‰æ‹©å™¨å¯åŠ¨çˆ¬è™«
response = requests.post("http://localhost:8000/api/v1/start-crawling", json={
    "url": "https://example-job-site.com/jobs",
    "selectors": selectors["selectors"],
    "options": {
        "max_pages": 10,  # æœ€å¤§çˆ¬å–é¡µæ•°
        "delay_between_pages": 2,  # é¡µé¢é—´å»¶è¿Ÿ(ç§’)
        "export_format": "json",  # å¯¼å‡ºæ ¼å¼: json, csv, excel
        "data_filters": {
            "location": ["åŒ—äº¬", "ä¸Šæµ·"],  # åœ°ç‚¹è¿‡æ»¤
            "keywords": ["Python", "AI"]  # å…³é”®è¯è¿‡æ»¤
        }
    }
})

session_info = response.json()
session_id = session_info["session_id"]
```

#### 3. ç›‘æ§çˆ¬å–è¿›åº¦

```python
import time

while True:
    # æŸ¥è¯¢çˆ¬å–çŠ¶æ€
    status = requests.get(f"http://localhost:8000/api/v1/crawl-status/{session_id}")
    status_data = status.json()

    print(f"çŠ¶æ€: {status_data['status']}")
    print(f"å·²çˆ¬å–é¡µé¢: {status_data['progress']['pages_crawled']}")
    print(f"å‘ç°èŒä½: {status_data['progress']['jobs_found']}")

    if status_data["status"] in ["completed", "failed"]:
        break

    time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
```

#### 4. ä¸‹è½½çˆ¬å–ç»“æœ

```python
if status_data["status"] == "completed":
    # ä¸‹è½½ç»“æœæ–‡ä»¶
    download_url = status_data["export_url"]
    file_response = requests.get(f"http://localhost:8000{download_url}")

    with open("jobs_data.json", "wb") as f:
        f.write(file_response.content)

    print("æ•°æ®ä¸‹è½½å®Œæˆ!")
```

### é«˜çº§ç”¨æ³•

#### è‡ªå®šä¹‰é€‰æ‹©å™¨é…ç½®

å¦‚æœ AI ç”Ÿæˆçš„é€‰æ‹©å™¨éœ€è¦è°ƒæ•´ï¼Œå¯ä»¥æ‰‹åŠ¨ä¿®æ”¹ï¼š

```python
# æ‰‹åŠ¨è°ƒæ•´é€‰æ‹©å™¨
custom_selectors = {
    "jobList": "div.search-results",  # ä¿®æ”¹èŒä½åˆ—è¡¨å®¹å™¨
    "jobItem": ".job-item",
    "jobTitle": ".job-title h3",      # æ›´ç²¾ç¡®çš„æ ‡é¢˜é€‰æ‹©å™¨
    "jobLink": ".job-title a",
    "companyName": ".company span",
    "publishedAt": ".posted-date",
    "location": ".location-info",
    "jobDescription": ".job-desc",
    "nextPage": "a[aria-label*='ä¸‹ä¸€é¡µ']"  # æ”¯æŒaria-label
}

# ä½¿ç”¨è‡ªå®šä¹‰é€‰æ‹©å™¨å¯åŠ¨çˆ¬è™«
response = requests.post("http://localhost:8000/api/v1/start-crawling", json={
    "url": url,
    "selectors": custom_selectors,
    "options": crawl_options
})
```

#### æ‰¹é‡ç½‘ç«™çˆ¬å–

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªç½‘ç«™
websites = [
    "https://jobs.site1.com/search",
    "https://jobs.site2.com/listings",
    "https://careers.site3.com/openings"
]

crawl_sessions = []

for url in websites:
    # 1. åˆ†ææ¯ä¸ªç½‘ç«™
    analysis = requests.post("http://localhost:8000/api/v1/analyze-url",
                           json={"url": url})

    if analysis.json()["confidence_score"] > 0.8:
        # 2. å¯åŠ¨çˆ¬è™«
        crawl_response = requests.post("http://localhost:8000/api/v1/start-crawling",
                                     json={
                                         "url": url,
                                         "selectors": analysis.json()["selectors"]
                                     })
        crawl_sessions.append(crawl_response.json()["session_id"])

# 3. ç›‘æ§æ‰€æœ‰ä»»åŠ¡
# ... ç›‘æ§é€»è¾‘
```

## ğŸ”§ é…ç½®é€‰é¡¹

### AI åˆ†æé…ç½®

```python
analyze_options = {
    "ai_model": "gpt-4-vision-preview",  # AIæ¨¡å‹é€‰æ‹©
    "confidence_threshold": 0.8,         # ç½®ä¿¡åº¦é˜ˆå€¼
    "max_analysis_time": 60,            # æœ€å¤§åˆ†ææ—¶é—´(ç§’)
    "include_visual_analysis": true,     # æ˜¯å¦åŒ…å«è§†è§‰åˆ†æ
    "fallback_to_heuristics": true      # æ˜¯å¦å›é€€åˆ°å¯å‘å¼æ–¹æ³•
}
```

### çˆ¬è™«é…ç½®

```python
crawl_options = {
    "max_pages": 50,                    # æœ€å¤§çˆ¬å–é¡µæ•°
    "max_jobs": 1000,                   # æœ€å¤§èŒä½æ•°é‡
    "delay_between_pages": (1, 3),      # é¡µé¢é—´éšæœºå»¶è¿Ÿ
    "timeout": 30,                      # é¡µé¢è¶…æ—¶æ—¶é—´
    "retry_count": 3,                   # é‡è¯•æ¬¡æ•°
    "export_format": "json",            # å¯¼å‡ºæ ¼å¼
    "data_quality_threshold": 0.7,      # æ•°æ®è´¨é‡é˜ˆå€¼
    "enable_screenshots": false,        # æ˜¯å¦ä¿å­˜æˆªå›¾
    "proxy_rotation": true,             # æ˜¯å¦è½®æ¢ä»£ç†
    "user_agent_rotation": true         # æ˜¯å¦è½®æ¢User-Agent
}
```

### åçˆ¬è™«é…ç½®

```python
anti_detection_config = {
    "stealth_mode": true,               # å¯ç”¨éšèº«æ¨¡å¼
    "human_behavior_simulation": true,  # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
    "random_delays": {                  # éšæœºå»¶è¿Ÿé…ç½®
        "page_load": (2, 5),
        "scroll": (1, 3),
        "click": (0.5, 1.5)
    },
    "proxy_settings": {                 # ä»£ç†è®¾ç½®
        "enabled": true,
        "rotation_interval": 10,        # è½®æ¢é—´éš”(é¡µé¢æ•°)
        "health_check": true
    }
}
```

## ğŸ“Š æ•°æ®æ ¼å¼

### æ ‡å‡†è¾“å‡ºæ ¼å¼

```json
{
  "metadata": {
    "crawl_session_id": "uuid-string",
    "source_url": "https://example.com/jobs",
    "crawl_timestamp": "2025-01-01T00:00:00Z",
    "total_pages": 5,
    "total_jobs": 125,
    "data_quality_score": 0.92
  },
  "jobs": [
    {
      "id": "job-uuid",
      "title": "é«˜çº§Pythonå¼€å‘å·¥ç¨‹å¸ˆ",
      "company": "ç§‘æŠ€åˆ›æ–°å…¬å¸",
      "location": "åŒ—äº¬å¸‚æœé˜³åŒº",
      "published_at": "2025-01-01",
      "description": "è´Ÿè´£åç«¯ç³»ç»Ÿå¼€å‘...",
      "link": "https://example.com/job/12345",
      "salary_range": "20k-35k",
      "experience_level": "3-5å¹´",
      "job_type": "å…¨èŒ",
      "skills_required": ["Python", "Django", "MySQL", "Redis"],
      "remote_option": false,
      "data_quality_score": 0.95,
      "extracted_at": "2025-01-01T00:05:23Z"
    }
  ],
  "pagination_info": {
    "current_page": 5,
    "total_pages": 5,
    "has_next": false,
    "next_page_url": null
  },
  "extraction_stats": {
    "success_rate": 0.94,
    "failed_extractions": 8,
    "duplicate_jobs": 3,
    "data_quality_issues": 5
  }
}
```

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### é¡¹ç›®ç»“æ„

```
crawler-assistant/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                    # APIè·¯ç”±
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ ai/                 # AIåˆ†ææ¨¡å—
â”‚   â”‚   â”œâ”€â”€ crawler/            # çˆ¬è™«å¼•æ“
â”‚   â”‚   â”œâ”€â”€ browser/            # Browser-useé›†æˆ
â”‚   â”‚   â””â”€â”€ data/               # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ models/                 # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ tests/                  # æµ‹è¯•ä»£ç 
â”œâ”€â”€ docs/                       # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ scripts/                    # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ docker-compose.yml          # Dockerç¼–æ’
â”œâ”€â”€ requirements.txt            # Pythonä¾èµ–
â””â”€â”€ README.md
```

### æœ¬åœ°å¼€å‘

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# å®‰è£…pre-commité’©å­
pre-commit install

# è¿è¡Œæµ‹è¯•
pytest

# è¿è¡Œæµ‹è¯•å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
pytest -v

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
pytest tests/test_basic.py -v

# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=. --cov-report=html --cov-report=term

# è¿è¡Œæµ‹è¯•æ—¶æ˜¾ç¤ºæ‰“å°è¾“å‡º
pytest -s

# è¿è¡Œå¤±è´¥æ—¶åœæ­¢
pytest -x

# ä»£ç æ ¼å¼åŒ–
black .
isort .

# ç±»å‹æ£€æŸ¥
mypy app/
```

### æ·»åŠ æ–°çš„é€‰æ‹©å™¨ç­–ç•¥

```python
# app/core/ai/selector_strategies.py

class CustomSelectorStrategy(BaseSelectorStrategy):
    """è‡ªå®šä¹‰é€‰æ‹©å™¨ç”Ÿæˆç­–ç•¥"""

    def generate_job_list_selector(self, dom_tree, analysis) -> str:
        # å®ç°ä½ çš„é€‰æ‹©å™¨ç”Ÿæˆé€»è¾‘
        pass

    def validate_selector(self, selector: str, dom_tree) -> float:
        # å®ç°é€‰æ‹©å™¨éªŒè¯é€»è¾‘
        pass

# æ³¨å†Œç­–ç•¥
SelectorStrategyRegistry.register("custom", CustomSelectorStrategy)
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. OpenAI API è°ƒç”¨å¤±è´¥

```bash
# æ£€æŸ¥APIå¯†é’¥
echo $OPENAI_API_KEY

# æµ‹è¯•APIè¿æ¥
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models
```

#### 2. æµè§ˆå™¨å¯åŠ¨å¤±è´¥

```bash
# å®‰è£…æµè§ˆå™¨ä¾èµ–
playwright install chromium

# æ£€æŸ¥Chromeæ˜¯å¦å¯ç”¨
google-chrome --version
```

#### 3. é€‰æ‹©å™¨å‡†ç¡®ç‡ä½

- æ£€æŸ¥ç›®æ ‡ç½‘ç«™çš„é¡µé¢ç»“æ„æ˜¯å¦å‘ç”Ÿå˜åŒ–
- å°è¯•é™ä½`confidence_threshold`
- æ‰‹åŠ¨è°ƒæ•´ç”Ÿæˆçš„é€‰æ‹©å™¨
- å¯ç”¨`fallback_to_heuristics`é€‰é¡¹

#### 4. æ€§èƒ½é—®é¢˜

```python
# è°ƒæ•´å¹¶å‘è®¾ç½®
crawl_options = {
    "max_concurrent_pages": 3,  # å‡å°‘å¹¶å‘æ•°
    "delay_between_requests": 2,  # å¢åŠ å»¶è¿Ÿ
    "enable_resource_blocking": true  # é˜»æ­¢ä¸å¿…è¦èµ„æº
}
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
docker-compose logs -f api

# æŸ¥çœ‹ç‰¹å®šä¼šè¯æ—¥å¿—
grep "session_id=xxx" /var/log/crawler-assistant/app.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f /var/log/crawler-assistant/error.log
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

### è´¡çŒ®æµç¨‹

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤ä¿®æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- [browser-use](https://github.com/gregpr07/browser-use) - å¼ºå¤§çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·
- [OpenAI](https://openai.com/) - GPT-4 AI æ¨¡å‹æ”¯æŒ
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£åŒ–çš„ Web æ¡†æ¶
- [Playwright](https://playwright.dev/) - å¯é çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–

## ğŸ“ æ”¯æŒ

- ğŸ“§ é‚®ç®±: support@example.com
- ğŸ’¬ å¾®ä¿¡ç¾¤: [æ‰«ç åŠ å…¥](qr-code-link)
- ğŸ“š æ–‡æ¡£: [åœ¨çº¿æ–‡æ¡£](https://docs.example.com)
- ğŸ› é—®é¢˜åé¦ˆ: [GitHub Issues](https://github.com/your-username/ai-crawler-assistant/issues)

---

<div align="center">
  <p>å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™å®ƒä¸€ä¸ª â­ï¸</p>
  <p>Made with â¤ï¸ by the AI Crawler Team</p>
</div>
